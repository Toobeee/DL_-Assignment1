# -*- coding: utf-8 -*-
"""DL_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vg7SE2CSeMCx_zIRCc7DgiaPHLJqeUtP
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

"""## Dataset"""

def generate_harder_spiral_data(points_per_class=200, noise=0.4, num_classes=3):
    N = points_per_class
    D = 2
    K = num_classes
    X = np.zeros((N*K, D))
    y = np.zeros(N*K, dtype='uint8')

    for j in range(K):
        ix = range(N*j, N*(j+1))
        r = np.linspace(0.0, 1, N)
        t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*noise
        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
        y[ix] = j

    return X, y

# Example usage
X, y = generate_harder_spiral_data()
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')
plt.title("Spiral Data")
plt.show()

# Generate data
X, y = generate_harder_spiral_data()

"""## Data Preparation"""

# Split into 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from tensorflow.keras.optimizers import Adam

"""## 2. Model Building"""

# 1. Define model
model = Sequential([
    InputLayer(shape=(2,)),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')
])

# 2. Compile model —
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

from tensorflow.keras.utils import to_categorical

y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

from tensorflow.keras.utils import to_categorical

# Assuming y is the original 1D label array (not one-hot)
X, y = generate_harder_spiral_data()
y_cat = to_categorical(y, num_classes=3)

# Now split the one-hot encoded labels
X_train, X_test, y_train, y_test = train_test_split(
    X, y_cat, test_size=0.2, random_state=42, stratify=y
)

print("y_train shape:", y_train.shape)  # Should be (num_samples, 3)

"""## Training"""

# 5. Train model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=500,
    batch_size=32
)

"""## 3. Model Evaluation"""

# Plot accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Accuracy over Epochs")
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss over Epochs")
plt.legend()

plt.tight_layout()
plt.show()

"""## Decision Boundry"""

def plot_decision_boundary(model, X, y):
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
    h = 0.01  # step size in mesh

    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    grid = np.c_[xx.ravel(), yy.ravel()]
    preds = model.predict(grid, verbose=0)
    preds_class = np.argmax(preds, axis=1)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, preds_class.reshape(xx.shape), cmap='viridis', alpha=0.5)
    plt.scatter(X[:, 0], X[:, 1], c=np.argmax(y, axis=1), cmap='viridis', edgecolor='k')
    plt.title("Decision Boundary")
    plt.xlabel("X1")
    plt.ylabel("X2")
    plt.show()

# Call the function using training data (or test data)
plot_decision_boundary(model, X, y_cat)

# 1. How did you decide the number of hidden layers and neurons?
# hidden layers, each with 64 neurons,  Activation: ReLU
#Spiral data is non-linearly separable I  started with a moderately complex architecture (2×64) to balance: sufficient capacity to learn patterns
# which Avoiding overfitting  If too few neurons/layers → underfitting. Too many → overfitting.


#2. How did different learning rates affect the results?
#used the Adam optimizer with a learning rate of 0.001, a common and stable default.
#Learning rate too high (e.g. 0.01): Training becomes unstable
#Loss fluctuates or diverges Learning rate too low (e.g. 0.0001):
#Model learns very slowly
#"A learning rate of 0.001 gave a good balance between speed and stability. Lower rates slowed convergence; higher rates caused overshooting"


#3.Did you encounter overfitting or underfitting? How did you deal with it?
#“Initially, we observed slight overfitting after 200+ epochs. To avoid this  tried reducing the model size and added early stopping.


#4. If you had more time, how would you further improve the model?
#Model Tuning Try different architectures (e.g., 3 layers, [128, 64, 32]) Use batch normalization or dropout Use learning rate schedules (e.g., ReduceLROnPlateau)

# Optimization Try different optimizers: SGD with momentum, RMSProp, etc.

#Visualization & Monitoring Add early stopping to avoid overfitting